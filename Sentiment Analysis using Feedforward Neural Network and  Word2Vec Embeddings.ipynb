{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c9255b5-32d5-4b81-becd-8a079fb38c4f",
   "metadata": {},
   "source": [
    "# **Sentiment Analysis using Feedforward Neural Network and Word2Vec Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf203d-110b-468b-9f21-45bc497f1d39",
   "metadata": {},
   "source": [
    "### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91dea301-b230-4752-b425-b3eceb94f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "import gensim.downloader as api\n",
    "import warnings\n",
    "\n",
    "nltk.download('stopwords',quiet = True)\n",
    "nltk.download('wordnet',quiet = True)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e60b9b-048f-4056-a09c-030859127039",
   "metadata": {},
   "source": [
    "### **Importing Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09841cb5-4da1-4a4c-897f-d5a669007bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./SentimentData.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf9a34f-76ae-416c-85c9-0c6fd58f6de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18203</th>\n",
       "      <td>It's a shame this movie didn't get more play i...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10677</th>\n",
       "      <td>It took us a couple of episodes to \"get into\" ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10249</th>\n",
       "      <td>Not the best of actors' movies.The director ha...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12053</th>\n",
       "      <td>This DVD usually sells for around $20. I would...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32502</th>\n",
       "      <td>I see a lot of folks on this site wishing AG w...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4587</th>\n",
       "      <td>Y'know, it's very interesting watching this......</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2861</th>\n",
       "      <td>\"The China Syndrome\" could not have been relea...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2202</th>\n",
       "      <td>Another Day - this movie requires you to watch...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27281</th>\n",
       "      <td>IT was no sense and it was so awful... i think...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41673</th>\n",
       "      <td>On one level, this film can bring out the chil...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "18203  It's a shame this movie didn't get more play i...  positive\n",
       "10677  It took us a couple of episodes to \"get into\" ...  positive\n",
       "10249  Not the best of actors' movies.The director ha...  negative\n",
       "12053  This DVD usually sells for around $20. I would...  positive\n",
       "32502  I see a lot of folks on this site wishing AG w...  positive\n",
       "4587   Y'know, it's very interesting watching this......  positive\n",
       "2861   \"The China Syndrome\" could not have been relea...  positive\n",
       "2202   Another Day - this movie requires you to watch...  negative\n",
       "27281  IT was no sense and it was so awful... i think...  negative\n",
       "41673  On one level, this film can bring out the chil...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f412b90b-6993-40fd-a8f5-556a501269fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce4ec45-50e0-4bd4-8d43-8de3156b9208",
   "metadata": {},
   "source": [
    "### **Text Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eef528de-c43c-41f7-a04a-0f5a42e6292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    #Removing special characters, punctuation, numbers, URLs, and extra spaces.\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)#Removes URLs from the given text.\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)#Removes special characters, punctuations, numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()#Removes extra spaces\n",
    "\n",
    "    #Converting all text to lower case.\n",
    "    text = text.lower()\n",
    "\n",
    "    #Splitting the text into individual tokens (words).\n",
    "    tokens = text.split()\n",
    "\n",
    "    #Removing common stopwords (for example, ”the”, ”of”, ”and”,...).\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    #Coverting verbs into their base forms.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "#Preprocessing the 'Reviews' in the data frame and storing them.\n",
    "data['preprocessed_reviews'] = data['review'].apply(preprocess_text)\n",
    "\n",
    "# Convert sentiment labels from categorical strings to numerical values\n",
    "# Mapping: 'positive' -> 1, 'negative' -> 0\n",
    "data['sentiment'] = data['sentiment'].map({'positive': 1, 'negative': 0})  # Adjust the mapping as needed based on your actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97433b64-ab93-4d7b-8878-49c3ef65d54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>preprocessed_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2901</th>\n",
       "      <td>Since my third or fourth viewing some time ago...</td>\n",
       "      <td>1</td>\n",
       "      <td>since third fourth viewing time ago ive abstai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>and anyone who watches this film will agree. T...</td>\n",
       "      <td>1</td>\n",
       "      <td>anyone watch film agree film directed day plot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19793</th>\n",
       "      <td>This is such a great film! Never mind the low ...</td>\n",
       "      <td>1</td>\n",
       "      <td>great film never mind low rating really idea c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27141</th>\n",
       "      <td>Corean cinema can be quite surprising for an o...</td>\n",
       "      <td>1</td>\n",
       "      <td>corean cinema quite surprising occidental audi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24946</th>\n",
       "      <td>Spoiler Alert Well I think this movie is proba...</td>\n",
       "      <td>0</td>\n",
       "      <td>spoiler alert well think movie probably worst ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34606</th>\n",
       "      <td>I find I enjoy this show, but the format needs...</td>\n",
       "      <td>1</td>\n",
       "      <td>find enjoy show format need work first good at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46070</th>\n",
       "      <td>alright this movie might have been good if the...</td>\n",
       "      <td>0</td>\n",
       "      <td>alright movie might good plot behind title did...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23116</th>\n",
       "      <td>I went to see this because of snipes / statham...</td>\n",
       "      <td>0</td>\n",
       "      <td>went see snipe statham honestly chaos terrible...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17114</th>\n",
       "      <td>Mean spirited, and down right degrading adapta...</td>\n",
       "      <td>0</td>\n",
       "      <td>mean spirited right degrading adaptation class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21232</th>\n",
       "      <td>if i could rate it a zero i would , coming fro...</td>\n",
       "      <td>0</td>\n",
       "      <td>could rate zero would coming someone like shoc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment  \\\n",
       "2901   Since my third or fourth viewing some time ago...          1   \n",
       "2800   and anyone who watches this film will agree. T...          1   \n",
       "19793  This is such a great film! Never mind the low ...          1   \n",
       "27141  Corean cinema can be quite surprising for an o...          1   \n",
       "24946  Spoiler Alert Well I think this movie is proba...          0   \n",
       "34606  I find I enjoy this show, but the format needs...          1   \n",
       "46070  alright this movie might have been good if the...          0   \n",
       "23116  I went to see this because of snipes / statham...          0   \n",
       "17114  Mean spirited, and down right degrading adapta...          0   \n",
       "21232  if i could rate it a zero i would , coming fro...          0   \n",
       "\n",
       "                                    preprocessed_reviews  \n",
       "2901   since third fourth viewing time ago ive abstai...  \n",
       "2800   anyone watch film agree film directed day plot...  \n",
       "19793  great film never mind low rating really idea c...  \n",
       "27141  corean cinema quite surprising occidental audi...  \n",
       "24946  spoiler alert well think movie probably worst ...  \n",
       "34606  find enjoy show format need work first good at...  \n",
       "46070  alright movie might good plot behind title did...  \n",
       "23116  went see snipe statham honestly chaos terrible...  \n",
       "17114  mean spirited right degrading adaptation class...  \n",
       "21232  could rate zero would coming someone like shoc...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ad1df-bf61-478b-bc15-e7f939da95bb",
   "metadata": {},
   "source": [
    "### **Word Embeddings with Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae0d0b70-ae01-41bb-9ad6-d5b7ea2b91bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>preprocessed_reviews</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29873</th>\n",
       "      <td>Joe Don Baker. He was great in \"Walking Tall\" ...</td>\n",
       "      <td>0</td>\n",
       "      <td>joe baker great walking tall good bitpart gold...</td>\n",
       "      <td>[0.029917685, 0.022685226, -0.013670604, 0.096...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14118</th>\n",
       "      <td>Leave it to Braik to put on a good show. Final...</td>\n",
       "      <td>1</td>\n",
       "      <td>leave braik put good show finally zorak living...</td>\n",
       "      <td>[0.04333496, 0.015791636, -0.0036208138, 0.095...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40808</th>\n",
       "      <td>When I first got wind of this picture, it was ...</td>\n",
       "      <td>1</td>\n",
       "      <td>first got wind picture called shepherd suppose...</td>\n",
       "      <td>[0.07394431, 0.05116824, -0.000601153, 0.06174...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37787</th>\n",
       "      <td>It is a truism that it takes a lot of effort t...</td>\n",
       "      <td>0</td>\n",
       "      <td>truism take lot effort make bad movie one exce...</td>\n",
       "      <td>[0.05006476, 0.0034094702, -0.04726294, 0.0843...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28839</th>\n",
       "      <td>Pretty poor Firestarter clone that seems more ...</td>\n",
       "      <td>0</td>\n",
       "      <td>pretty poor firestarter clone seems like bad t...</td>\n",
       "      <td>[0.09375922, 0.030691028, -0.01133128, 0.08002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30304</th>\n",
       "      <td>So umm this woman has a vagina that sucks peop...</td>\n",
       "      <td>0</td>\n",
       "      <td>umm woman vagina suck people umm there dude li...</td>\n",
       "      <td>[0.048493944, 0.016780438, 0.010638511, 0.1359...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14224</th>\n",
       "      <td>Ever since I started visiting this site, and v...</td>\n",
       "      <td>0</td>\n",
       "      <td>ever since started visiting site voting movie ...</td>\n",
       "      <td>[0.05767489, 0.027181417, 0.010050765, 0.12420...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42373</th>\n",
       "      <td>I rented this movie simply because Rosario Daw...</td>\n",
       "      <td>1</td>\n",
       "      <td>rented movie simply rosario dawson sat watch b...</td>\n",
       "      <td>[0.052316885, 0.037958153, 0.015223141, 0.0830...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28154</th>\n",
       "      <td>The most amazing, spiritually uplifting movie ...</td>\n",
       "      <td>1</td>\n",
       "      <td>amazing spiritually uplifting movie restoratio...</td>\n",
       "      <td>[0.09076538, 0.018694958, -0.022803752, 0.1261...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24277</th>\n",
       "      <td>**1/2 for this Diane Keaton farce.&lt;br /&gt;&lt;br /&gt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>diane keaton farcebr br someone tell m keaton ...</td>\n",
       "      <td>[0.03529368, 0.027727127, -0.005451353, 0.0747...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment  \\\n",
       "29873  Joe Don Baker. He was great in \"Walking Tall\" ...          0   \n",
       "14118  Leave it to Braik to put on a good show. Final...          1   \n",
       "40808  When I first got wind of this picture, it was ...          1   \n",
       "37787  It is a truism that it takes a lot of effort t...          0   \n",
       "28839  Pretty poor Firestarter clone that seems more ...          0   \n",
       "30304  So umm this woman has a vagina that sucks peop...          0   \n",
       "14224  Ever since I started visiting this site, and v...          0   \n",
       "42373  I rented this movie simply because Rosario Daw...          1   \n",
       "28154  The most amazing, spiritually uplifting movie ...          1   \n",
       "24277  **1/2 for this Diane Keaton farce.<br /><br />...          0   \n",
       "\n",
       "                                    preprocessed_reviews  \\\n",
       "29873  joe baker great walking tall good bitpart gold...   \n",
       "14118  leave braik put good show finally zorak living...   \n",
       "40808  first got wind picture called shepherd suppose...   \n",
       "37787  truism take lot effort make bad movie one exce...   \n",
       "28839  pretty poor firestarter clone seems like bad t...   \n",
       "30304  umm woman vagina suck people umm there dude li...   \n",
       "14224  ever since started visiting site voting movie ...   \n",
       "42373  rented movie simply rosario dawson sat watch b...   \n",
       "28154  amazing spiritually uplifting movie restoratio...   \n",
       "24277  diane keaton farcebr br someone tell m keaton ...   \n",
       "\n",
       "                                              embeddings  \n",
       "29873  [0.029917685, 0.022685226, -0.013670604, 0.096...  \n",
       "14118  [0.04333496, 0.015791636, -0.0036208138, 0.095...  \n",
       "40808  [0.07394431, 0.05116824, -0.000601153, 0.06174...  \n",
       "37787  [0.05006476, 0.0034094702, -0.04726294, 0.0843...  \n",
       "28839  [0.09375922, 0.030691028, -0.01133128, 0.08002...  \n",
       "30304  [0.048493944, 0.016780438, 0.010638511, 0.1359...  \n",
       "14224  [0.05767489, 0.027181417, 0.010050765, 0.12420...  \n",
       "42373  [0.052316885, 0.037958153, 0.015223141, 0.0830...  \n",
       "28154  [0.09076538, 0.018694958, -0.022803752, 0.1261...  \n",
       "24277  [0.03529368, 0.027727127, -0.005451353, 0.0747...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the pre-trained Word2Vec model (Google News)\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Function to create embeddings\n",
    "def get_review_embedding(review):\n",
    "    words = review.split()  # Assuming the reviews are space-separated\n",
    "    word_vectors = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in model:  # Check if the word is in the model\n",
    "            word_vectors.append(model[word])\n",
    "    \n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)  # Average vector\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Zero vector if no words are found\n",
    "\n",
    "# Apply the function to create word embeddings\n",
    "data['embeddings'] = data['preprocessed_reviews'].apply(get_review_embedding)\n",
    "\n",
    "#Displaying the data frame after creating embeddings for processed reviews\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19cde18f-3e11-4c9d-9e10-ec6a269b50bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['embeddings'][10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3c1916-24cd-4bb5-9e21-b69b098e105e",
   "metadata": {},
   "source": [
    "### **Splitting the dataset into training, testing and validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b96d623-d0f3-4e40-84fa-450a79a02957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataset\n",
    "X = np.array(data['embeddings'].tolist())\n",
    "y = np.array(data['sentiment'])\n",
    "# Split the dataset\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f43ac0-8e69-4df4-8a04-08d8dc3a19ac",
   "metadata": {},
   "source": [
    "### **Building and training Feed Forward Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeb3b4f5-0135-4967-a9a3-18e0ae04af4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7796 - loss: 0.4375 - val_accuracy: 0.8165 - val_loss: 0.4153\n",
      "Epoch 2/20\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8426 - loss: 0.3619 - val_accuracy: 0.8397 - val_loss: 0.3622\n",
      "Epoch 3/20\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8550 - loss: 0.3365 - val_accuracy: 0.8492 - val_loss: 0.3419\n",
      "Epoch 4/20\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8561 - loss: 0.3305 - val_accuracy: 0.8231 - val_loss: 0.3984\n",
      "Epoch 5/20\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8544 - loss: 0.3356 - val_accuracy: 0.8393 - val_loss: 0.3573\n",
      "Epoch 6/20\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8586 - loss: 0.3251 - val_accuracy: 0.8508 - val_loss: 0.3361\n",
      "Epoch 7/20\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8655 - loss: 0.3170 - val_accuracy: 0.8508 - val_loss: 0.3354\n",
      "Epoch 8/20\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8689 - loss: 0.3068 - val_accuracy: 0.8532 - val_loss: 0.3494\n",
      "Epoch 9/20\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8699 - loss: 0.3067 - val_accuracy: 0.8467 - val_loss: 0.3623\n",
      "Epoch 10/20\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8755 - loss: 0.2946 - val_accuracy: 0.8376 - val_loss: 0.3680\n",
      "Epoch 11/20\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8693 - loss: 0.3054 - val_accuracy: 0.8523 - val_loss: 0.3367\n",
      "Epoch 12/20\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8813 - loss: 0.2820 - val_accuracy: 0.8520 - val_loss: 0.3369\n",
      "Epoch 13/20\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8801 - loss: 0.2783 - val_accuracy: 0.8509 - val_loss: 0.3347\n",
      "Epoch 14/20\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8827 - loss: 0.2745 - val_accuracy: 0.8559 - val_loss: 0.3446\n",
      "Epoch 15/20\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8868 - loss: 0.2653 - val_accuracy: 0.8561 - val_loss: 0.3390\n",
      "Epoch 16/20\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8913 - loss: 0.2554 - val_accuracy: 0.8575 - val_loss: 0.3546\n",
      "Epoch 17/20\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8931 - loss: 0.2522 - val_accuracy: 0.8493 - val_loss: 0.3548\n",
      "Epoch 18/20\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8971 - loss: 0.2431 - val_accuracy: 0.8416 - val_loss: 0.3882\n",
      "Epoch 19/20\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8993 - loss: 0.2350 - val_accuracy: 0.8540 - val_loss: 0.3600\n",
      "Epoch 20/20\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9049 - loss: 0.2245 - val_accuracy: 0.8473 - val_loss: 0.3702\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(300,)),  # Adjust for your embedding size\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=20,\n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe280dcd-efde-47ed-8d25-ed0c411ab407",
   "metadata": {},
   "source": [
    "### **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98522c86-151a-4a40-8f01-5d5bfd780dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Accuracy on D_test: 0.8476\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8524 - loss: 0.3698\n",
      "Test Loss: 0.3782\n",
      "Test Accuracy: 0.8476\n"
     ]
    }
   ],
   "source": [
    "# Get predicted probabilities\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# True labels (ensure y_test is in the same format)\n",
    "# Assuming y_test is already binary (0 or 1)\n",
    "\n",
    "# Calculate the accuracy\n",
    "correct_predictions = np.sum(y_pred.flatten() == y_test.flatten())  # Count correct predictions\n",
    "T = len(y_test)  # Total number of instances in the test set\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / T\n",
    "\n",
    "print(f\"Accuracy on D_test: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
